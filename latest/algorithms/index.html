<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Algorithms · TheGraphOpt.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://semiotic-ai.github.io/TheGraphOpt.jl/algorithms/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">TheGraphOpt.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Algorithms</a><ul class="internal"><li><a class="tocitem" href="#Gradient-Descent"><span>Gradient Descent</span></a></li><li><a class="tocitem" href="#Projected-Gradient-Descent"><span>Projected Gradient Descent</span></a></li><li><a class="tocitem" href="#Halpern-Iteration"><span>Halpern Iteration</span></a></li></ul></li><li><a class="tocitem" href="../hooks/">Hooks</a></li><li><a class="tocitem" href="../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Algorithms</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Algorithms</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/semiotic-ai/TheGraphOpt.jl/blob/main/docs/src/algorithms.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Algorithms"><a class="docs-heading-anchor" href="#Algorithms">Algorithms</a><a id="Algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithms" title="Permalink"></a></h1><p>This section will introduce you to the basic workflow that you&#39;ll use to optimise functions, as well as the algorithms we&#39;ve implemented.</p><p>Generally speaking, there are two parts to optimising. The first is to specify the function you want to optimise. Say,</p><pre><code class="language-julia hljs">julia&gt; f(x) = sum(x .^ 2)</code></pre><p>Then you want to create a <a href="../hooks/#hooks">Hook</a> with the <a href="../hooks/#TheGraphOpt.IsStoppingCondition"><code>TheGraphOpt.IsStoppingCondition</code></a> trait. Else, optimisation will get stuck in an infinite loop.</p><pre><code class="language-julia hljs">julia&gt; using LinearAlgebra
julia&gt; h = StopWhen((a; locals...) -&gt; norm(x(a) - locals[:z]) &lt; 1e-6)  # Stop when the residual is less than the tolerance</code></pre><p>Then, you want to choose the algorithm you want to use for minimisation and specify its parameters.</p><pre><code class="language-julia hljs">julia&gt; a = GradientDescent(; x=[100.0, 50.0], η=1e-1, hooks=[h,])  # Specify parameters for optimisation</code></pre><p>Finally, you&#39;ll run <code>minimize!</code> or <code>minimize</code>.</p><pre><code class="language-julia hljs">julia&gt; sol = minimize!(f, a)  # Optimise</code></pre><article class="docstring"><header><a class="docstring-binding" id="TheGraphOpt.minimize!" href="#TheGraphOpt.minimize!"><code>TheGraphOpt.minimize!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">minimize!(f::Function, a::OptAlgorithm)</code></pre><p>Minimize <code>f</code> using <code>a</code>.</p><p>Does in-place updates of <code>a.x</code>. This will generally be more performant than <a href="#TheGraphOpt.minimize"><code>TheGraphOpt.minimize</code></a>. However, there are cases in which this will be worse, so we provide both.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you don&#39;t provide any hook with the <a href="../hooks/#TheGraphOpt.IsStoppingCondition"><code>IsStoppingCondition</code></a> trait, this will loop forever.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/semiotic-ai/TheGraphOpt.jl/blob/cb91acef084cce15c3021314a2e6b7a6c3d442ec/src/core.jl#L20-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TheGraphOpt.minimize" href="#TheGraphOpt.minimize"><code>TheGraphOpt.minimize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">minimize(f::Function, a::OptAlgorithm)</code></pre><p>Minimize <code>f</code> using <code>a</code>.</p><p>This will generally be less performant than <a href="#TheGraphOpt.minimize!"><code>TheGraphOpt.minimize!</code></a>. However, there are cases in which this will be better, so we provide it as an option.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you don&#39;t provide any hook with the <a href="../hooks/#TheGraphOpt.IsStoppingCondition"><code>IsStoppingCondition</code></a> trait, this will loop forever.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/semiotic-ai/TheGraphOpt.jl/blob/cb91acef084cce15c3021314a2e6b7a6c3d442ec/src/core.jl#L6-L17">source</a></section></article><p><code>sol</code> here is a struct containing various metadata. If you only care about the optimal value of <code>x</code>, then grab it using</p><pre><code class="language-julia hljs">julia&gt; TheGraphOpt.x(sol)</code></pre><h2 id="Gradient-Descent"><a class="docs-heading-anchor" href="#Gradient-Descent">Gradient Descent</a><a id="Gradient-Descent-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Descent" title="Permalink"></a></h2><p>From <a href="https://en.wikipedia.org/wiki/Gradient_descent">Wikipedia</a>:</p><blockquote><p>[Gradient Descent]... is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. </p></blockquote><p>If the function we&#39;re optimising <code>f</code> is convex, then a local minimum is also a global minimum. The update rule for gradient descent is <span>$x_{n+1}=x_n - η∇f(x_n)$</span>.</p><article class="docstring"><header><a class="docstring-binding" id="TheGraphOpt.GradientDescent" href="#TheGraphOpt.GradientDescent"><code>TheGraphOpt.GradientDescent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GradientDescent(;x::V, η::T, hooks::S) where {
    T&lt;:Real,V&lt;:AbstractVector{T},S&lt;:AbstractVecOrTuple{&lt;:Hook}
}</code></pre><p>Parameters for gradient descent learning.</p><p><strong>Fields</strong></p><ul><li><code>η::T</code> is the learning rate/step size.</li><li><code>x::V</code> is the current best guess for the solution.</li><li><code>hooks::S</code> are the hooks</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/semiotic-ai/TheGraphOpt.jl/blob/cb91acef084cce15c3021314a2e6b7a6c3d442ec/src/gradientdescent.jl#L6-L17">source</a></section></article><h2 id="Projected-Gradient-Descent"><a class="docs-heading-anchor" href="#Projected-Gradient-Descent">Projected Gradient Descent</a><a id="Projected-Gradient-Descent-1"></a><a class="docs-heading-anchor-permalink" href="#Projected-Gradient-Descent" title="Permalink"></a></h2><p>Projected Gradient Descent is a more general gradient descent in a sense. Whereas gradient descent itself has no constraint, projected gradient descent allows for you to specify a constraint. In particular, here, we are interested in solving the problem of <span>$\min_x f(x)$</span> subject to some constraints. Those constraints define the feasible region <span>$\mathcal{C}$</span>. Thus, the full problem we&#39;re trying to solve is <span>$\min_x f(x) \textrm{subject to } x\in\mathcal{C}$</span>. Once we do the standard gradient descent step, here we project the result onto the feasible set. In other words, </p><p class="math-container">\[\begin{align*}
    y_{n} &amp;= x_n - η∇f(x_n) \\
    x_{n+1} &amp;= \textrm{Pr}(y_n)_\mathcal{C}
\end{align*}\]</p><p>To use PGD, you&#39;ll need to specify <code>t</code>, the projection function. Any projection function must take only <code>x</code> as input. To get more complex behaviour, you may find it useful to leverage currying (partial function application).</p><p>Let&#39;s look at an example of this. Say we want to minimise <span>$f(x)=\sum x^2$</span> subject to a unit simplex constraint. We provide a function <a href="#TheGraphOpt.σsimplex"><code>TheGraphOpt.σsimplex</code></a>. However, this function doesn&#39;t only take <code>x</code> as input, but also takes <code>σ</code>. In order to create a projection function in terms of only <code>x</code>, we will apply currying by creating a new function.</p><pre><code class="language-julia hljs">julia&gt; using TheGraphOpt
julia&gt; unitsimplex(x) = σsimplex(x, 1)</code></pre><p>Now, we can create a PGD parameters struct and solve our problem.</p><pre><code class="language-julia hljs">julia&gt; using LinearAlgebra
julia&gt; f(x) = sum(x.^2)
julia&gt; a = ProjectedGradientDescent(;
            x=[100.0, 50.0],
            η=1e-1,
            hooks=[StopWhen((a; kws...) -&gt; norm(x(a) - kws[:z]) &lt; 1e-6)],
            t=unitsimplex,
        )
julia&gt; aopt = minimize(f, a)
julia&gt; TheGraphOpt.x(aopt)
2-element Vector{Float64}:
 0.5000023384026198
 0.49999766159738035</code></pre><article class="docstring"><header><a class="docstring-binding" id="TheGraphOpt.ProjectedGradientDescent" href="#TheGraphOpt.ProjectedGradientDescent"><code>TheGraphOpt.ProjectedGradientDescent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ProjectedGradientDescent(;
    x::AbstractVector{T}, η::T, hooks::AbstractVecOrTuple{&lt;:Hook}, t::F
) where {T&lt;:Real,F&lt;:Function}
ProjectedGradientDescent(g::G, t::F) where {G&lt;:GradientDescent,F&lt;:Function}</code></pre><p>Specifies parameters for <a href="#TheGraphOpt.GradientDescent"><code>TheGraphOpt.GradientDescent</code></a> and the projection function <code>t</code>.</p><p><code>t</code> takes as input the vector to be projected <code>x</code> and returns the projected vector.</p><p><strong>Forwarded Methods</strong></p><ul><li><a href="../api/#TheGraphOpt.x"><code>TheGraphOpt.x</code></a></li><li><a href="../api/#TheGraphOpt.η"><code>TheGraphOpt.η</code></a></li><li><a href="../api/#TheGraphOpt.hooks"><code>TheGraphOpt.hooks</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/semiotic-ai/TheGraphOpt.jl/blob/cb91acef084cce15c3021314a2e6b7a6c3d442ec/src/projectedgradientdescent.jl#L6-L20">source</a></section></article><h3 id="Predefined-Projection-Functions"><a class="docs-heading-anchor" href="#Predefined-Projection-Functions">Predefined Projection Functions</a><a id="Predefined-Projection-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Predefined-Projection-Functions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="TheGraphOpt.σsimplex" href="#TheGraphOpt.σsimplex"><code>TheGraphOpt.σsimplex</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">σsimplex(x::AbstractVector{T}, σ::Real) where {T&lt;:Real}</code></pre><p>Project <code>x</code> onto the <code>σ</code>-simplex.</p><p>In other words, project <code>x</code>s to be non-negative and sum to <code>σ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/semiotic-ai/TheGraphOpt.jl/blob/cb91acef084cce15c3021314a2e6b7a6c3d442ec/src/project.jl#L6-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TheGraphOpt.gssp" href="#TheGraphOpt.gssp"><code>TheGraphOpt.gssp</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gssp(x::AbstractVector{&lt;:Real}, k::Int, σ::Real)</code></pre><p>Project <code>x</code> onto the intersection of the set of <code>k</code>-sparse vectors and the <code>σ</code>-simplex.</p><p>Reference: http://proceedings.mlr.press/v28/kyrillidis13.pdf</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/semiotic-ai/TheGraphOpt.jl/blob/cb91acef084cce15c3021314a2e6b7a6c3d442ec/src/project.jl#L22-L28">source</a></section></article><h2 id="Halpern-Iteration"><a class="docs-heading-anchor" href="#Halpern-Iteration">Halpern Iteration</a><a id="Halpern-Iteration-1"></a><a class="docs-heading-anchor-permalink" href="#Halpern-Iteration" title="Permalink"></a></h2><p><a href="https://projecteuclid.org/journals/bulletin-of-the-american-mathematical-society/volume-73/issue-6/Fixed-points-of-nonexpanding-maps/bams/1183529119.pdf">Halpern iteration</a> is a method of anchoring an iterative optimisation algorithm to some anchor point <span>$x_0$</span>. The general form of the algorithm is given by</p><p class="math-container">\[x_{k+1} = \lambda_{k+1}x_0 + (1 - \lambda_{k+1})T(x_k)\]</p><p>Here, <span>$T: \mathbb{R}^d \to \mathbb{R}^d$</span> is a non-expansive map (i.e., <span>$\forall x,y \in \mathbb{R}^d: ||T(x) - T(y)|| \leq ||x - y||$</span>). <span>$\lambda_k$</span> is a step size that must be chosen such that it satisfies the properties.</p><p class="math-container">\[\begin{align*}
    \lim_{k\to\infty} \lambda_k &amp;= 0 \\
    \sum_{k=1}^\infty \lambda_k &amp;= \infty \\
    \sum_{k=1}^\infty |\lambda_{k+1} - \lambda_k| &amp;&lt; \infty \\
\end{align*}\]</p><p>A reasonable first guess for <span>$\lambda_k$</span> might be <span>$\frac{1}{k}$</span> if you&#39;re unsure about what to pick.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>In the formula for Halpern Iteration, we actually use <span>$k+1$</span>. In our code, you should still define <span>$\lambda_k$</span> as we&#39;ll add the one for you.</p></div></div><p>This is an implicitly regularised method. If you use an algorithm like gradient descent with Halpern Iteration, you&#39;ll converge to the solution with the minimum <span>$\ell2$</span> distance from <code>x₀</code>.</p><p>Implementation-wise, we&#39;ve actually defined Halpern Iteration using a <a href="../hooks/#hooks">Hook</a> that exhibits the <a href="../hooks/#TheGraphOpt.RunAfterIteration"><code>TheGraphOpt.RunAfterIteration</code></a> trait. To use this, you&#39;ll want to add this hook to an existing algorithm. For example,</p><pre><code class="language-julia hljs">julia&gt; using LinearAlgebra
julia&gt; f(x) = sum(x.^2)
julia&gt; a = GradientDescent(;
            x=[100.0, 50.0],
            η=1e-1,
            hooks=[
                StopWhen((a; kws...) -&gt; norm(x(a) - kws[:z]) &lt; 1e-6),
                HalpernIteration(; x₀=[10, 10], λ=k -&gt; 1 / k),
            ],
        )
julia&gt; o = minimize!(f, a)
julia&gt; x(o)
2-element Vector{Float64}:
 0.005945303210463734
 0.005945303210463734</code></pre><article class="docstring"><header><a class="docstring-binding" id="TheGraphOpt.HalpernIteration" href="#TheGraphOpt.HalpernIteration"><code>TheGraphOpt.HalpernIteration</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HalpernIteration{T&lt;:Real,V&lt;:AbstractVector{T}} &lt;: Hook</code></pre><p>A hook for using <a href="https://projecteuclid.org/journals/bulletin-of-the-american-mathematical-society/volume-73/issue-6/Fixed-points-of-nonexpanding-maps/bams/1183529119.pdf">Halpern Iteration</a>. in which you should specify the <code>x₀</code> and <code>λ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/semiotic-ai/TheGraphOpt.jl/blob/cb91acef084cce15c3021314a2e6b7a6c3d442ec/src/halperniteration.jl#L6-L11">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../hooks/">Hooks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 19 October 2022 20:59">Wednesday 19 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
