var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"This section documents all functions not documented elsewhere","category":"page"},{"location":"api/","page":"API","title":"API","text":"TheGraphOpt.x\nTheGraphOpt.x!\nTheGraphOpt.η\nTheGraphOpt.hooks\nTheGraphOpt.maybeminimize!\nTheGraphOpt.shouldstop\nTheGraphOpt.iteration\nTheGraphOpt.postiteration","category":"page"},{"location":"api/#TheGraphOpt.x","page":"API","title":"TheGraphOpt.x","text":"x(g::GradientDescent)\nx(g::GradientDescent, v)\n\nThe current best guess for the solution. If using the setter, v is the new value.\n\nThe setter is not in-place. See TheGraphOpt.x!.\n\n\n\n\n\nx(g::ProjectedGradientDescent)\nx(g::ProjectedGradientDescent, v)\n\nThe current best guess for the solution. If using the setter, v is the new value.\n\nThe setter is not in-place. See TheGraphOpt.x!.\n\n\n\n\n\n","category":"function"},{"location":"api/#TheGraphOpt.x!","page":"API","title":"TheGraphOpt.x!","text":"x!(g::GradientDescent, v)\n\nIn-place setting of g.x to v\n\nSee TheGraphOpt.x.\n\n\n\n\n\nx!(g::ProjectedGradientDescent, v)\n\nIn-place setting of a.g.x to v\n\nSee TheGraphOpt.x.\n\n\n\n\n\n","category":"function"},{"location":"api/#TheGraphOpt.η","page":"API","title":"TheGraphOpt.η","text":"η(g::GradientDescent)\n\nThe learning rate/step size.\n\n\n\n\n\n","category":"function"},{"location":"api/#TheGraphOpt.hooks","page":"API","title":"TheGraphOpt.hooks","text":"hooks(g::GradientDescent)\n\nThe hooks used by the algorithm.\n\n\n\n\n\n","category":"function"},{"location":"api/#TheGraphOpt.maybeminimize!","page":"API","title":"TheGraphOpt.maybeminimize!","text":"maybeminimize!(f::Function, a::OptAlgorithm, op::Function)\n\nMinimize f using a, which calls op for updating a.x.\n\nThis function may be in-place. Don't use it directly unless you know what you're doing. This function is unexported.\n\nwarning: Warning\nIf you don't provide any hook with the IsStoppingCondition trait, this will loop forever.\n\n\n\n\n\n","category":"function"},{"location":"api/#TheGraphOpt.shouldstop","page":"API","title":"TheGraphOpt.shouldstop","text":"shouldstop(hs::AbstractVecOrTuple{H}, a::OptAlgorithm; locals...)\n\nMap over the hooks hs to check for stopping conditions.\n\nThis performs an OR operation if there are multiple hooks that have the TheGraphOpt.IsStoppingCondition trait.\n\n\n\n\n\n","category":"function"},{"location":"api/#TheGraphOpt.iteration","page":"API","title":"TheGraphOpt.iteration","text":"iteration(f::Function, a::GradientDescent)\n\nOne iteration of a on the function f.\n\nThis function is unexported.\n\n\n\n\n\niteration(f::Function, a::ProjectedGradientDescent)\n\nApply a to f, projected based on a.t.\n\n\n\n\n\n","category":"function"},{"location":"api/#TheGraphOpt.postiteration","page":"API","title":"TheGraphOpt.postiteration","text":"postiterationhook(\n    hs::AbstractVecOrTuple{H}, a::OptAlgorithm, z::AbstractVector{T}; locals...\n) where {H<:Hook,T<:Real}\n\nRun hooks that the code should execute after TheGraphOpt.iteration.\n\n\n\n\n\n","category":"function"},{"location":"hooks/#hooks","page":"Hooks","title":"Hooks","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Hooks enable you to dynamically choose to inject functionality into algorithms. Some hooks are mandatory for certain algorithms, such as having a hook with the TheGraphOpt.IsStoppingCondition trait when using TheGraphOpt.GradientDescent. Others are purely there for you to use at your discretion. In this section, we'll take you through Traits, Using Predefined Hooks and how to Create Custom Hooks.","category":"page"},{"location":"hooks/#Traits","page":"Hooks","title":"Traits","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Hooks have traits. This is how our code knows when to execute which hook. For example, the code will execute a hook that has the TheGraphOpt.IsStoppingCondition trait when evaluating whether it has finished optimising. A full list of traits follows.","category":"page"},{"location":"hooks/#StopTrait","page":"Hooks","title":"StopTrait","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"This trait tells the code if it should execute a hook when checking stopping conditions. By default, the code automatically gives all hooks the TheGraphOpt.IsNotStoppingCondition trait except in certain situations, which are explicitly documented with the hooks in question. We decide to stop code execution on an OR basis. This means that if any hook with TheGraphOpt.IsStoppingCondition returns true, the code breaks out of the optimisation loop. Hooks with this trait must implement TheGraphOpt.stophook.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"TheGraphOpt.StopTrait\nTheGraphOpt.IsStoppingCondition\nTheGraphOpt.IsNotStoppingCondition\nTheGraphOpt.stophook","category":"page"},{"location":"hooks/#TheGraphOpt.StopTrait","page":"Hooks","title":"TheGraphOpt.StopTrait","text":"Abstract type for stopping conditions traits\n\n\n\n\n\n","category":"type"},{"location":"hooks/#TheGraphOpt.IsStoppingCondition","page":"Hooks","title":"TheGraphOpt.IsStoppingCondition","text":"Exhibited by hooks that are stopping conditions.\n\nStopping condition hooks must emit a boolean value when TheGraphOpt.stophook is called. If multiple hooks meet IsStoppingCondition are instantiated at the same time, we assume that they are meant to be OR'ed, so if any of them is true, optimisation is finished. For more complex behaviour, consider defining a more complex function using a single StopWhen.\n\nTo set this trait for a hook, run\n\njulia> StopTrait(::Type{MyHook}) = IsStoppingCondition()\n\n\n\n\n\n","category":"type"},{"location":"hooks/#TheGraphOpt.IsNotStoppingCondition","page":"Hooks","title":"TheGraphOpt.IsNotStoppingCondition","text":"Exhibited by hooks that are not stopping conditions.\n\nThis is the default case. You should not have to set it manually for any hooks.\n\n\n\n\n\n","category":"type"},{"location":"hooks/#TheGraphOpt.stophook","page":"Hooks","title":"TheGraphOpt.stophook","text":"stophook(::IsStoppingCondition, h::Hook, a::OptAlgorithm; locals...)\n\nRaise an error if the hook is a stopping condition but has not implemented TheGraphOpt.stophook.\n\n\n\n\n\nstophook(h::IsNotStoppingCondition, a::OptAlgorithm; locals...)\n\nIf the hook isn't a stopping condition, it shouldn't be considered in the OR, so return false.\n\n\n\n\n\nstophook(::IsStoppingCondition, h::StopWhen, a::OptAlgorithm; locals...)\n\nCall h.f on a and ;locals.\n\n\n\n\n\n","category":"function"},{"location":"hooks/#PostIterationTrait","page":"Hooks","title":"PostIterationTrait","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"This trait tells the code if it should execute the hook after it calls TheGraphOpt.iteration. All hooks default to the TheGraphOpt.DontRunAfterIteration trait unless otherwise documented. Hooks with this positive variant of this trait TheGraphOpt.RunAfterIteration must return z, the output of iteration. Hooks with this trait must also implement TheGraphOpt.postiterationhook.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"TheGraphOpt.PostIterationTrait\nTheGraphOpt.RunAfterIteration\nTheGraphOpt.DontRunAfterIteration\nTheGraphOpt.postiterationhook","category":"page"},{"location":"hooks/#TheGraphOpt.PostIterationTrait","page":"Hooks","title":"TheGraphOpt.PostIterationTrait","text":"Abstract type for trait denoting hooks that should run after an iteration finishes.\n\n\n\n\n\n","category":"type"},{"location":"hooks/#TheGraphOpt.RunAfterIteration","page":"Hooks","title":"TheGraphOpt.RunAfterIteration","text":"Exhibited by hooks that run after an iteration finishes.\n\nSuch hooks must take the output of TheGraphOpt.iteration z as input and return z back, potentially modified.\n\nTo set this trait for a hook, run\n\njulia> PostIterationTrait(::Type{MyHook}) = RunAfterIteration()\n\n\n\n\n\n","category":"type"},{"location":"hooks/#TheGraphOpt.DontRunAfterIteration","page":"Hooks","title":"TheGraphOpt.DontRunAfterIteration","text":"Exhibited by hooks that don't run after an iteration finishes.\n\nThis is the default case. You should not have to set it manually for any hooks.\n\n\n\n\n\n","category":"type"},{"location":"hooks/#TheGraphOpt.postiterationhook","page":"Hooks","title":"TheGraphOpt.postiterationhook","text":"postiterationhook(\n    ::RunAfterIteration, h::Hook, a::OptAlgorithm, z::AbstractVector{T}; locals...\n) where {T<:Real}\n\nRaise an error if the hook exhibits TheGraphOpt.RunAfterIteration but has not implemented TheGraphOpt.postiterationhook.\n\n\n\n\n\npostiterationhook(     ::DontRunAfterIteration, h::Hook, a::OptAlgorithm, z::AbstractVector{T}; locals... ) where {T<:Real}\n\nIf the hook shouldn't run after an iteration, just return z unmodified.\n\n\n\n\n\n","category":"function"},{"location":"hooks/#Using-Predefined-Hooks","page":"Hooks","title":"Using Predefined Hooks","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Hooks always descend from the same abstract type.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"TheGraphOpt.Hook","category":"page"},{"location":"hooks/#TheGraphOpt.Hook","page":"Hooks","title":"TheGraphOpt.Hook","text":"Abstract type for hooks.\n\n\n\n\n\n","category":"type"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Generally speaking, pre-defined hooks won't exhibit any positive traits unless explicitly documented otherwise. This serves two purposes. Firstly, it prevents any unexpected behaviour when the code executes. Secondly, it gives you more flexibility when choosing where you want a hook to be executed.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"warning: Warning\nThis section is incomplete! We will fill this out more once we have more traits and hooks to work with.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"note: Note\nWe recommend you read the below StopWhen section as it explains details that we won't cover in the later sections since it'd be too repetitive.","category":"page"},{"location":"hooks/#StopWhen","page":"Hooks","title":"StopWhen","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"This hook has the TheGraphOpt.IsStoppingCondition trait. To use it, you would specify a function that returns a boolean value. If said value is true, then the code breaks out of the optimisation loop.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Let's take an example from our tests to demonstrate how to specify this hook. We'll also implement a dummy optimisation function that just implements a counter for illustration purposes.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"julia> using TheGraphOpt\njulia> struct FakeOptAlg <: TheGraphOpt.OptAlgorithm end\njuila> a = FakeOptAlg()\njulia> function counter(h, a)\n            i = 0\n            while !shouldstop(h, a; Base.@locals()...)\n                i += 1\n            end\n            return i\n        end\njulia> h = StopWhen((a; locals...) -> locals[:i] ≥ 5, Dict())  # Stop when i ≥ 5\njulia> i = counter((h,), a)\n5","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"One thing you may not have seen is Base.@locals. This takes variables from the local scope (in this case, from the counter scope), and tracks them as a dictionary of symbols. Thus, since i is a local variable inside of counter, :i becomes a key in the Base.@locals dictionary. We pass this dictionary to the anonymouse function stored by StopWhen. Then, we can use locals[:i] to get the value of i from the counter scope and check it against some condition. This is a powerful trick you may find yourself using a lot when dealing with hooks.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"TheGraphOpt.StopWhen","category":"page"},{"location":"hooks/#TheGraphOpt.StopWhen","page":"Hooks","title":"TheGraphOpt.StopWhen","text":"StopWhen(f::Function)\n\nStops optimisation when some condition is met.\n\nThe condition is set by f. Note that f gets access to variables in the TheGraphOpt.minimize scope. This means, for example, that it can use locals[:z] to compute residuals. This has the TheGraphOpt.IsStoppingCondition trait.\n\n\n\n\n\n","category":"type"},{"location":"hooks/#Create-Custom-Hooks","page":"Hooks","title":"Create Custom Hooks","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"When you create a custom hook, you need to follow three steps. The first is that you need to descend from TheGraphOpt.Hook.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"julia> using TheGraphOpt\njulia> struct MyHook <: Hook end","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"The second is that you need to ensure that you specify which traits you want that hook to exhibit. For example, let's say MyHook is a stopping condition. You'd want to implement.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"julia> StopTrait(::Type{MyHook}) = IsStoppingCondition()","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Finally, if you need non-default behaviour for when the hook executes, you'll need to implement whatever function(s) the code calls for that trait-type. For IsStoppingCondition, that's stophook. Say we want MyHook to immediately cause optimisation to finish. We'd implement","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"julia> stophook(h::MyHook, a::TheGraphOpt.OptAlgorithm; locals...) = true","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"That's it! As long as your follow those three steps, you should be able to implement whatever hook you want!","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = TheGraphOpt","category":"page"},{"location":"#TheGraphOpt","page":"Home","title":"TheGraphOpt","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements optimisation algorithms for use within simulation and production. For the most part, your workflow should look something like","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using TheGraphOpt\njulia> using LinearAlgebra\njulia> f(x) = sum(x .^ 2)  # Specify function to optimise as min f(x)\njulia> a = GradientDescent(;\n            x=[100.0, 50.0],  # Specify parameters for optimisation\n            η=1e-1,\n            hooks=[StopWhen((a; kws...) -> norm(x(a) - kws[:z]) < 1e-6)],  # hook stops opt when residual is below 1e-6.\n        )\njulia> sol = minimize!(f, a)  # Optimise\njulia> @show TheGraphOpt.x(sol)  # Print out the optimal value\n2-element Vector{Float64}:\n 3.4163644416613304e-6\n 1.6909278549636878e-6","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Make sure you've installed Julia 1.8 or greater.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is hosted on SemioticJLRegistry. To add this package, first add the registry to your Julia installation. Then, install this package by running ] add TheGraphOpt from the Julia REPL.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]registry add https://github.com/semiotic-ai/SemioticJLRegistry\njulia> ]add TheGraphOpt","category":"page"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"This section will introduce you to the basic workflow that you'll use to optimise functions, as well as the algorithms we've implemented.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Generally speaking, there are two parts to optimising. The first is to specify the function you want to optimise. Say,","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> f(x) = sum(x .^ 2)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Then you want to create a Hook with the TheGraphOpt.IsStoppingCondition trait. Else, optimisation will get stuck in an infinite loop.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> using LinearAlgebra\njulia> h = StopWhen((a; locals...) -> norm(x(a) - locals[:z]) < 1e-6)  # Stop when the residual is less than the tolerance","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Then, you want to choose the algorithm you want to use for minimisation and specify its parameters.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> a = GradientDescent(; x=[100.0, 50.0], η=1e-1, hooks=[h,])  # Specify parameters for optimisation","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Finally, you'll run minimize! or minimize.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> sol = minimize!(f, a)  # Optimise","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"TheGraphOpt.minimize!\nTheGraphOpt.minimize","category":"page"},{"location":"algorithms/#TheGraphOpt.minimize!","page":"Algorithms","title":"TheGraphOpt.minimize!","text":"minimize!(f::Function, a::OptAlgorithm)\n\nMinimize f using a.\n\nDoes in-place updates of a.x. This will generally be more performant than TheGraphOpt.minimize. However, there are cases in which this will be worse, so we provide both.\n\nwarning: Warning\nIf you don't provide any hook with the IsStoppingCondition trait, this will loop forever.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#TheGraphOpt.minimize","page":"Algorithms","title":"TheGraphOpt.minimize","text":"minimize(f::Function, a::OptAlgorithm)\n\nMinimize f using a.\n\nThis will generally be less performant than TheGraphOpt.minimize!. However, there are cases in which this will be better, so we provide it as an option.\n\nwarning: Warning\nIf you don't provide any hook with the IsStoppingCondition trait, this will loop forever.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"sol here is a struct containing various metadata. If you only care about the optimal value of x, then grab it using","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> TheGraphOpt.x(sol)","category":"page"},{"location":"algorithms/#Gradient-Descent","page":"Algorithms","title":"Gradient Descent","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"From Wikipedia:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"[Gradient Descent]... is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"If the function we're optimising f is convex, then a local minimum is also a global minimum. The update rule for gradient descent is x_n+1=x_n - ηf(x_n).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"TheGraphOpt.GradientDescent","category":"page"},{"location":"algorithms/#TheGraphOpt.GradientDescent","page":"Algorithms","title":"TheGraphOpt.GradientDescent","text":"GradientDescent(;x::V, η::T, hooks::S) where {\n    T<:Real,V<:AbstractVector{T},S<:AbstractVecOrTuple{<:Hook}\n}\n\nParameters for gradient descent learning.\n\nFields\n\nη::T is the learning rate/step size.\nx::V is the current best guess for the solution.\nhooks::S are the hooks\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Projected-Gradient-Descent","page":"Algorithms","title":"Projected Gradient Descent","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Projected Gradient Descent is a more general gradient descent in a sense. Whereas gradient descent itself has no constraint, projected gradient descent allows for you to specify a constraint. In particular, here, we are interested in solving the problem of min_x f(x) subject to some constraints. Those constraints define the feasible region mathcalC. Thus, the full problem we're trying to solve is min_x f(x) textrmsubject to  xinmathcalC. Once we do the standard gradient descent step, here we project the result onto the feasible set. In other words, ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"beginalign*\n    y_n = x_n - ηf(x_n) \n    x_n+1 = textrmPr(y_n)_mathcalC\nendalign*","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"To use PGD, you'll need to specify t, the projection function. Any projection function must take only x as input. To get more complex behaviour, you may find it useful to leverage currying (partial function application).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Let's look at an example of this. Say we want to minimise f(x)=sum x^2 subject to a unit simplex constraint. We provide a function TheGraphOpt.σsimplex. However, this function doesn't only take x as input, but also takes σ. In order to create a projection function in terms of only x, we will apply currying by creating a new function.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> using TheGraphOpt\njulia> unitsimplex(x) = σsimplex(x, 1)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Now, we can create a PGD parameters struct and solve our problem.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> using LinearAlgebra\njulia> f(x) = sum(x.^2)\njulia> a = ProjectedGradientDescent(;\n            x=[100.0, 50.0],\n            η=1e-1,\n            hooks=[StopWhen((a; kws...) -> norm(x(a) - kws[:z]) < 1e-6)],\n            t=unitsimplex,\n        )\njulia> aopt = minimize(f, a)\njulia> TheGraphOpt.x(aopt)\n2-element Vector{Float64}:\n 0.5000023384026198\n 0.49999766159738035","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"TheGraphOpt.ProjectedGradientDescent","category":"page"},{"location":"algorithms/#TheGraphOpt.ProjectedGradientDescent","page":"Algorithms","title":"TheGraphOpt.ProjectedGradientDescent","text":"ProjectedGradientDescent(;\n    x::AbstractVector{T}, η::T, hooks::AbstractVecOrTuple{<:Hook}, t::F\n) where {T<:Real,F<:Function}\nProjectedGradientDescent(g::G, t::F) where {G<:GradientDescent,F<:Function}\n\nSpecifies parameters for TheGraphOpt.GradientDescent and the projection function t.\n\nt takes as input the vector to be projected x and returns the projected vector.\n\nForwarded Methods\n\nTheGraphOpt.x\nTheGraphOpt.η\nTheGraphOpt.hooks\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Predefined-Projection-Functions","page":"Algorithms","title":"Predefined Projection Functions","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"TheGraphOpt.σsimplex\nTheGraphOpt.gssp","category":"page"},{"location":"algorithms/#TheGraphOpt.σsimplex","page":"Algorithms","title":"TheGraphOpt.σsimplex","text":"σsimplex(x::AbstractVector{T}, σ::Real) where {T<:Real}\n\nProject x onto the σ-simplex.\n\nIn other words, project xs to be non-negative and sum to σ.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#TheGraphOpt.gssp","page":"Algorithms","title":"TheGraphOpt.gssp","text":"gssp(x::AbstractVector{<:Real}, k::Int, σ::Real)\n\nProject x onto the intersection of the set of k-sparse vectors and the σ-simplex.\n\nReference: http://proceedings.mlr.press/v28/kyrillidis13.pdf\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Halpern-Iteration","page":"Algorithms","title":"Halpern Iteration","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Halpern iteration is a method of anchoring an iterative optimisation algorithm to some anchor point x_0. The general form of the algorithm is given by","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"x_k+1 = lambda_k+1x_0 + (1 - lambda_k+1)T(x_k)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Here, T mathbbR^d to mathbbR^d is a non-expansive map (i.e., forall xy in mathbbR^d T(x) - T(y) leq x - y). lambda_k is a step size that must be chosen such that it satisfies the properties.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"beginalign*\n    lim_ktoinfty lambda_k = 0 \n    sum_k=1^infty lambda_k = infty \n    sum_k=1^infty lambda_k+1 - lambda_k  infty \nendalign*","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A reasonable first guess for lambda_k might be frac1k if you're unsure about what to pick.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"note: Note\nIn the formula for Halpern Iteration, we actually use k+1. In our code, you should still define lambda_k as we'll add the one for you.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"This is an implicitly regularised method. If you use an algorithm like gradient descent with Halpern Iteration, you'll converge to the solution with the minimum ell2 distance from x₀.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Implementation-wise, we've actually defined Halpern Iteration using a Hook that exhibits the TheGraphOpt.RunAfterIteration trait. To use this, you'll want to add this hook to an existing algorithm. For example,","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> using LinearAlgebra\njulia> f(x) = sum(x.^2)\njulia> a = GradientDescent(;\n            x=[100.0, 50.0],\n            η=1e-1,\n            hooks=[\n                StopWhen((a; kws...) -> norm(x(a) - kws[:z]) < 1e-6),\n                HalpernIteration(; x₀=[10, 10], λ=k -> 1 / k),\n            ],\n        )\njulia> o = minimize!(f, a)\njulia> x(o)\n2-element Vector{Float64}:\n 0.005945303210463734\n 0.005945303210463734","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"TheGraphOpt.HalpernIteration","category":"page"},{"location":"algorithms/#TheGraphOpt.HalpernIteration","page":"Algorithms","title":"TheGraphOpt.HalpernIteration","text":"HalpernIteration{T<:Real,V<:AbstractVector{T}} <: Hook\n\nA hook for using Halpern Iteration. in which you should specify the x₀ and λ.\n\n\n\n\n\n","category":"type"}]
}
