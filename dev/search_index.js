var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = TheGraphOpt","category":"page"},{"location":"#TheGraphOpt","page":"Home","title":"TheGraphOpt","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements optimisation algorithms for use within simulation and production. For the most part, your workflow should look something like","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using TheGraphOpt\njulia> f(x) = sum(x .^ 2)  # Specify function to optimise as min f(x)\njulia> a = GradientDescent(; x=[100.0, 50.0], η=1e-1, ϵ=1e-6)  # Specify parameters for optimisation\njulia> sol = minimize!(f, a)  # Optimise\njulia> @show TheGraphOpt.x(sol)  # Print out the optimal value\n2-element Vector{Float64}:\n 3.4163644416613304e-6\n 1.6909278549636878e-6","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Make sure you've installed Julia 1.8 or greater.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is hosted on SemioticJLRegistry. To add this package, first add the registry to your Julia installation. Then, install this package by running ] add TheGraphOpt from the Julia REPL.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]registry add https://github.com/semiotic-ai/SemioticJLRegistry\njulia> ]add TheGraphOpt","category":"page"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"This section will introduce you to the basic workflow that you'll use to optimise functions, as well as the algorithms we've implemented.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Generally speaking, there are two parts to optimising. The first is to specify the function you want to optimise. Say,","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> f(x) = sum(x .^ 2)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Then, you want to choose the algorithm you want to use for minimisation and specify its parameters.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> a = GradientDescent(; x=[100.0, 50.0], η=1e-1, ϵ=1e-6)  # Specify parameters for optimisation","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Finally, you'll run minimize! or minimize.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> sol = minimize!(f, a)  # Optimise","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"TheGraphOpt.minimize!\nTheGraphOpt.minimize","category":"page"},{"location":"algorithms/#TheGraphOpt.minimize!","page":"Algorithms","title":"TheGraphOpt.minimize!","text":"minimize!(f::Function, a::OptAlgorithm)\n\nMinimize f using a.\n\nDoes in-place updates of a.x. This will generally be more performant than TheGraphOpt.minimize. However, there are cases in which this will be worse, so we provide both.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#TheGraphOpt.minimize","page":"Algorithms","title":"TheGraphOpt.minimize","text":"minimize(f::Function, a::OptAlgorithm)\n\nMinimize f using a.\n\nThis will generally be less performant than TheGraphOpt.minimize!. However, there are cases in which this will be better, so we provide it as an option.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"sol here is a struct containing various metadata. If you only care about the optimal value of x, then grab it using","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> TheGraphOpt.x(sol)","category":"page"},{"location":"algorithms/#Algorithms-2","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/#Gradient-Descent","page":"Algorithms","title":"Gradient Descent","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"From Wikipedia:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"[Gradient Descent]... is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"If the function we're optimising f is convex, then a local minimum is also a global minimum. The update rule for gradient descent is x_n+1=x_n - ηf(x_n).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"TheGraphOpt.GradientDescent","category":"page"},{"location":"algorithms/#TheGraphOpt.GradientDescent","page":"Algorithms","title":"TheGraphOpt.GradientDescent","text":"GradientDescent{T<:Real}(x::Vector{T}, η::T)\n\nSpecifies parameters for gradient descent learning.\n\nη is the learning rate/step size. x is the current best guess for the solution. ϵ is the tolerance.\n\n\n\n\n\n","category":"type"}]
}
